{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bedfed0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2025-08-12 09:16:53.841536] --- Benchmarking ORIGINAL model ---\n",
      "Compiled circuit with rank None on device cuda\n",
      "logits shape: torch.Size([1, 256, 256])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-3, 2], but got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 119\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# Calculate Normalizer Z\u001b[39;00m\n\u001b[32m    118\u001b[39m iq_orig = IntegrateQuery(original_circuit)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m Z_bok_orig = \u001b[43miq_orig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintegrate_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mScope\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_circuit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# # Reduce to get final log normalizer logZ\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# logZ_orig = complex_logsumexp(Z_bok_orig, dim=-1)\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# logZ_orig = complex_logsumexp(logZ_orig, dim=-1)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    131\u001b[39m \n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Get circuit output and compute NLL\u001b[39;00m\n\u001b[32m    133\u001b[39m circuit_output_real = original_circuit(batch).real\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitrepos/msc-cirkit/cirkit/backend/torch/queries.py:104\u001b[39m, in \u001b[36mIntegrateQuery.__call__\u001b[39m\u001b[34m(self, x, integrate_vars)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m integrate_vars_mask.shape[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m1\u001b[39m, x.shape[\u001b[32m0\u001b[39m]):\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     98\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe number of scopes to integrate over must \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33meither match the batch size of x, or be 1 if you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwant to broadcast. Found #inputs = \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mintegrate_vars_mask.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m = len(integrate_vars)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    102\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_circuit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunctools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mIntegrateQuery\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_layer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintegrate_vars_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mintegrate_vars_mask\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (O, B, K)\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitrepos/msc-cirkit/cirkit/backend/torch/graph/modules.py:333\u001b[39m, in \u001b[36mTorchDiAcyclicGraph.evaluate\u001b[39m\u001b[34m(self, x, module_fn)\u001b[39m\n\u001b[32m    331\u001b[39m         y = module(*inputs)\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m         y = \u001b[43mmodule_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m     module_outputs.append(y)\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe address book is malformed\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitrepos/msc-cirkit/cirkit/backend/torch/queries.py:142\u001b[39m, in \u001b[36mIntegrateQuery._layer_fn\u001b[39m\u001b[34m(layer, x, integrate_vars_mask)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.any(integration_mask).item():\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m integration_output = \u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# Use the integration mask to select which output should be the result of\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# an integration operation, and which should not be\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# This is done in parallel for all folds, and regardless of whether the\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# circuit is folded or unfolded\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.where(integration_mask, integration_output, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitrepos/msc-cirkit/cirkit/backend/torch/layers/input.py:283\u001b[39m, in \u001b[36mTorchExpFamilyLayer.integrate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mintegrate\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     log_partition = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_partition_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.semiring.map_from(log_partition, LSESumSemiring)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitrepos/msc-cirkit/cirkit/backend/torch/layers/input.py:419\u001b[39m, in \u001b[36mTorchCategoricalLayer.log_partition_function\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;66;03m#return torch.logsumexp(self.logits(), dim=2).unsqueeze(1)\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlogits shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.sum(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m, dim=\u001b[32m2\u001b[39m).unsqueeze(dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: Dimension out of range (expected to be in range of [-3, 2], but got 3)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "import faulthandler\n",
    "\n",
    "# Assuming the necessary components from cirkit and your project are available\n",
    "from src.circuit_types import CIRCUIT_BUILDERS\n",
    "from cirkit.symbolic.circuit import Circuit\n",
    "from cirkit.pipeline import PipelineContext, compile as compile_circuit\n",
    "import cirkit.symbolic.functional as SF\n",
    "from cirkit.backend.torch.queries import IntegrateQuery\n",
    "from cirkit.utils.scope import Scope\n",
    "from cirkit.backend.torch.layers import TorchSumLayer\n",
    "from src.nystromlayer import NystromSumLayer\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def compile_symbolic(circuit: Circuit, *, device: str, rank: int | None = None, opt: bool = False):\n",
    "    \"\"\"Compile a symbolic circuit with optional Nyström optimization.\"\"\"\n",
    "    ctx = PipelineContext(\n",
    "        backend=\"torch\",\n",
    "        semiring=\"complex-lse-sum\",\n",
    "        fold=False,\n",
    "        optimize=opt,\n",
    "        nystrom_rank=rank,\n",
    "    )\n",
    "    compiled = compile_circuit(circuit, ctx, nystrom_rank=rank).to(device).eval()\n",
    "    print(f\"Compiled circuit with rank {rank} on device {device}\", flush=True)\n",
    "    return compiled\n",
    "\n",
    "\n",
    "def sync_sumlayer_weights(\n",
    "    original: nn.Module, nystrom: nn.Module, *, pivot: str = \"uniform\", rank: int | None = None\n",
    ") -> None:\n",
    "    \"\"\"Copy weights from ``original`` to ``nystrom`` for matching layers.\"\"\"\n",
    "    orig_layers = [m for m in original.modules() if isinstance(m, TorchSumLayer)]\n",
    "    nys_layers = [m for m in nystrom.modules() if isinstance(m, NystromSumLayer)]\n",
    "    if len(orig_layers) != len(nys_layers):\n",
    "        print(f\"{len(orig_layers)},{len(nys_layers)}\")\n",
    "        raise ValueError(\"Layer count mismatch when syncing weights\")\n",
    "\n",
    "    import faulthandler\n",
    "    faulthandler.enable(all_threads=True)\n",
    "    total = len(orig_layers)\n",
    "    interval = max(1, total // 4)\n",
    "\n",
    "    for i, (o, n) in enumerate(zip(orig_layers, nys_layers), start=1):\n",
    "        start = time.perf_counter()\n",
    "        faulthandler.dump_traceback_later(30, repeat=False)\n",
    "        try:\n",
    "            if rank is not None:\n",
    "                n.rank = int(rank)\n",
    "                n.rank_param.data.fill_(n.rank)\n",
    "            n.pivot = pivot\n",
    "            n._build_factors_from(o)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "        except Exception as e:\n",
    "            print(f\"[{datetime.now()}] Exception on layer {i}/{total}: {e}\", flush=True)\n",
    "            raise\n",
    "        finally:\n",
    "            faulthandler.cancel_dump_traceback_later()\n",
    "\n",
    "        if i % interval == 0 or i == total:\n",
    "            pct = int(100 * i / total)\n",
    "            print(f\"[{datetime.now()}] Weight Sync Progress: {pct}% ({i}/{total})\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "def complex_logsumexp(z, dim):\n",
    "    \"\"\"Numerically stable complex log-sum-exp, corrected for broadcasting.\"\"\"\n",
    "    # Keep dimension for stable subtraction\n",
    "    m = z.real.max(dim=dim, keepdim=True).values\n",
    "    \n",
    "    # Calculate the log-sum-exp part\n",
    "    log_sum_exp_part = (z - m).exp().sum(dim=dim).log()\n",
    "    \n",
    "    # Squeeze `m` to match the shape of `log_sum_exp_part` for correct addition\n",
    "    return log_sum_exp_part + m.squeeze(dim)\n",
    "\n",
    "# --- Setup and Data Loading ---\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "physical_batch_size = 16\n",
    "n_input = 28 * 28\n",
    "\n",
    "# Build the symbolic circuit\n",
    "builder = CIRCUIT_BUILDERS[\"MNIST\"]\n",
    "builder_kwargs = {\"num_input_units\": 16, \"num_sum_units\": 16, \"region_graph\": \"quad-tree-4\"}\n",
    "symbolic = builder(**builder_kwargs)\n",
    "squared = SF.multiply(symbolic, symbolic)\n",
    "\n",
    "# Prepare the MNIST test data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: (255 * x.view(-1)).long())])\n",
    "data_test = datasets.MNIST(root=\"./.data\", train=False, download=True, transform=transform)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=physical_batch_size)\n",
    "batch, _ = next(iter(test_dataloader))\n",
    "batch = batch.to(device)\n",
    "\n",
    "# --- 1. Original Model NLL Calculation ---\n",
    "\n",
    "print(f\"\\n[{datetime.now()}] --- Benchmarking ORIGINAL model ---\", flush=True)\n",
    "original_circuit = compile_symbolic(squared, device=device, rank=None, opt=False)\n",
    "\n",
    "# Load the pretrained model from cache\n",
    "units = 16\n",
    "cache_path = f\"./model_cache/checkpoints/mnist_{units}_{units}_epoch10.pt\"\n",
    "if os.path.exists(cache_path):\n",
    "    original_circuit.load_state_dict(torch.load(cache_path, map_location=device)[\"model_state_dict\"])\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Checkpoint not found at {cache_path}\")\n",
    "\n",
    "# Calculate Normalizer Z\n",
    "iq_orig = IntegrateQuery(original_circuit)\n",
    "Z_bok_orig = iq_orig(batch, integrate_vars=Scope(original_circuit.scope))\n",
    "\n",
    "\n",
    "# Get circuit output and compute NLL\n",
    "circuit_output_real = original_circuit(batch).real\n",
    "nll_orig = -(circuit_output_real - Z_bok_orig[0][0].real)\n",
    "\n",
    "print(\"\\n--- NLL Calculation for Original Model ---\")\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "print(f\"Normalizer Z_bok shape: {Z_bok_orig.shape}\")\n",
    "print(f\"Circuit Output shape: {circuit_output_real.shape}\")\n",
    "print(f\"NLL shape: {nll_orig.shape}\")\n",
    "print(f\"Average NLL for the Original batch: {nll_orig.mean().item()}\")\n",
    "\n",
    "# --- 2. Nyström Approximated Model NLL Calculation ---\n",
    "\n",
    "print(f\"\\n[{datetime.now()}] --- Benchmarking NYSTRÖM model ---\", flush=True)\n",
    "nystrom_circuit = compile_symbolic(squared, device=device, rank=254, opt=True)\n",
    "\n",
    "\n",
    "# Synchronize weights from the trained original model\n",
    "sync_sumlayer_weights(original_circuit, nystrom_circuit, pivot=\"l2\", rank=63)\n",
    "\n",
    "# Calculate Normalizer Z for the Nystrom model\n",
    "iq_nys = IntegrateQuery(nystrom_circuit)\n",
    "Z_bok_nys = iq_nys(batch, integrate_vars=Scope(nystrom_circuit.scope))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get circuit output and compute NLL, APPLYING THE SAME FIX\n",
    "nystrom_output_real = nystrom_circuit(batch).real\n",
    "nll_nys = -(nystrom_output_real - Z_bok_nys[0][0].real)\n",
    "\n",
    "print(\"\\n--- NLL Calculation for Nyström Approximated Model ---\")\n",
    "print(f\"Normalizer Z_bok_nys shape: {Z_bok_nys.shape}\")\n",
    "print(f\"Nyström Output shape: {nystrom_output_real.shape}\")\n",
    "print(f\"NLL Nyström shape: {nll_nys.shape}\")\n",
    "print(f\"Average NLL for the Nyström batch: {nll_nys.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b529a0-eb3b-409d-8eed-7e68086f4d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
