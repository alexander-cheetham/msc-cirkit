{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d7e1412-0df5-417a-aef3-e9b6b268323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from cirkit.symbolic.circuit import Circuit\n",
    "from cirkit.templates.region_graph import RandomBinaryTree, RegionGraph\n",
    "from cirkit.symbolic.layers import GaussianLayer\n",
    "from cirkit.symbolic.parameters import mixing_weight_factory\n",
    "from cirkit.templates.utils import Parameterization, parameterization_to_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ee1d7a-bb68-43c8-8a03-c3a63ac35f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.pipeline import compile as cirkit_compile\n",
    "from torch.cuda.amp import autocast, GradScaler        # mixed precision (optional)\n",
    "from cirkit.pipeline import compile\n",
    "from cirkit.pipeline import PipelineContext\n",
    "import functools\n",
    "from cirkit.symbolic.layers import HadamardLayer,SumLayer\n",
    "from cirkit.templates.utils import InputLayerFactory,SumLayerFactory,ProductLayerFactory\n",
    "from cirkit.symbolic.parameters import ParameterFactory\n",
    "from typing import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e58a23-0e9a-4146-9bca-4fb4c5eedf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_circuit2(\n",
    "        self,\n",
    "        *,\n",
    "        input_factory: InputLayerFactory,\n",
    "        sum_product: str | None = None,\n",
    "        sum_weight_factory: ParameterFactory | None = None,\n",
    "        nary_sum_weight_factory: ParameterFactory | None = None,\n",
    "        sum_factory: SumLayerFactory | None = None,\n",
    "        prod_factory: ProductLayerFactory | None = None,\n",
    "        num_input_units: int = 1,\n",
    "        num_sum_units: int = 1,\n",
    "        num_classes: int = 1,\n",
    "        factorize_multivariate: bool = True,\n",
    "    ) -> Circuit:\n",
    "        \"\"\"Construct a symbolic circuit from a region graph.\n",
    "            There are two ways to use this method. The first one is to specify a sum-product layer\n",
    "            abstraction, which can be either 'cp' (the CP layer), 'cp-t' (the CP-transposed layer),\n",
    "            or 'tucker' (the Tucker layer). The second one is to manually specify the factories to\n",
    "            build distinct um and product layers. If the first way is chosen, then one can possibly\n",
    "            use a factory that builds the symbolic parameters of the sum-product layer abstractions.\n",
    "            The factory that constructs the input factory must always be specified.\n",
    "\n",
    "        Args:\n",
    "            input_factory: A factory that builds an input layer.\n",
    "            sum_product: The sum-product layer to use. It can be None, 'cp', 'cp-t', or 'tucker'.\n",
    "            sum_weight_factory: The factory to construct the weights of the sum layers.\n",
    "                It can be None, or a parameter factory, i.e., a map\n",
    "                from a shape to a symbolic parameter. If it is None, then the default\n",
    "                weight factory of the sum layer is used instead.\n",
    "            nary_sum_weight_factory: The factory to construct the weight of sum layers havity arity\n",
    "                greater than one. If it is None, then it will have the same value and semantics of\n",
    "                the given sum_weight_factory.\n",
    "            sum_factory: A factory that builds a sum layer. It can be None.\n",
    "            prod_factory: A factory that builds a product layer. It can be None.\n",
    "            num_input_units: The number of input units.\n",
    "            num_sum_units: The number of sum units per sum layer.\n",
    "            num_classes: The number of output classes.\n",
    "            factorize_multivariate: Whether to fully factorize input layers, when they depend on\n",
    "                more than one variable.\n",
    "\n",
    "        Returns:\n",
    "            Circuit: A symbolic circuit.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: If an unknown 'sum_product' is given.\n",
    "            ValueError: If both 'sum_product' and layer factories are specified, or none of them.\n",
    "            ValueError: If 'sum_product' is specified, but 'weight_factory' is not.\n",
    "            ValueError: The given region graph is malformed.\n",
    "        \"\"\"\n",
    "        if (sum_factory is None and prod_factory is not None) or (\n",
    "            sum_factory is not None and prod_factory is None\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Both 'sum_factory' and 'prod_factory' must be specified or none of them\"\n",
    "            )\n",
    "        if sum_product is None and (sum_factory is None or prod_factory is None):\n",
    "            raise ValueError(\n",
    "                \"If 'sum_product' is not given, then both 'sum_factory' and 'prod_factory'\"\n",
    "                \" must be specified\"\n",
    "            )\n",
    "        if sum_product is not None and (sum_factory is not None or prod_factory is not None):\n",
    "            raise ValueError(\n",
    "                \"At most one between 'sum_product' and the pair 'sum_factory' and 'prod_factory'\"\n",
    "                \" must be specified\"\n",
    "            )\n",
    "        if nary_sum_weight_factory is None:\n",
    "            nary_sum_weight_factory = sum_weight_factory\n",
    "\n",
    "        layers: list[Layer] = []\n",
    "        in_layers: dict[Layer, list[Layer]] = {}\n",
    "        node_to_layer: dict[RegionGraphNode, Layer] = {}\n",
    "\n",
    "        def build_cp_(\n",
    "            rgn: RegionNode, rgn_partitioning: Sequence[RegionNode]\n",
    "        ) -> HadamardLayer | SumLayer:\n",
    "            layer_ins = [node_to_layer[rgn_in] for rgn_in in rgn_partitioning]\n",
    "            # fetch the already‐built layers for each child\n",
    "        \n",
    "            # sanity check: make sure all child outputs have the same dimensionality\n",
    "            dims = [l.num_output_units for l in layer_ins]\n",
    "            assert len(set(dims)) == 1, \"Can't element-wise sum different dims\"\n",
    "        \n",
    "            # build one SumLayer that takes BOTH inputs at once\n",
    "            sum_layer = SumLayer(\n",
    "                num_input_units = dims[0],       # input vector size\n",
    "                num_output_units = num_sum_units,# your chosen output size\n",
    "                arity = len(layer_ins),          # here, 2\n",
    "                weight_factory = sum_weight_factory\n",
    "            )\n",
    "        \n",
    "            # record it in your bookkeeping\n",
    "            layers.append(sum_layer)\n",
    "            in_layers[sum_layer] = layer_ins\n",
    "            node_to_layer[rgn] = sum_layer\n",
    "        \n",
    "            return sum_layer\n",
    "\n",
    "        def build_cp_transposed_(\n",
    "            rgn: RegionNode, rgn_partitioning: Sequence[RegionNode]\n",
    "        ) -> SumLayer:\n",
    "            layer_ins = [node_to_layer[rgn_in] for rgn_in in rgn_partitioning]\n",
    "            num_in_units = list({li.num_output_units for li in layer_ins})\n",
    "            if len(num_in_units) > 1:\n",
    "                raise ValueError(\n",
    "                    \"Cannot build a CP transposed layer, as the inputs would have different units\"\n",
    "                )\n",
    "            num_units = num_sum_units if self.region_outputs(rgn) else num_classes\n",
    "            hadamard = HadamardLayer(num_in_units[0], arity=len(rgn_partitioning))\n",
    "            dense = SumLayer(num_in_units[0], num_units, weight_factory=sum_weight_factory)\n",
    "            layers.append(hadamard)\n",
    "            layers.append(dense)\n",
    "            in_layers[hadamard] = layer_ins\n",
    "            in_layers[dense] = [hadamard]\n",
    "            node_to_layer[rgn] = dense\n",
    "            return dense\n",
    "\n",
    "        def build_tucker_(rgn: RegionNode, rgn_partitioning: Sequence[RegionNode]) -> SumLayer:\n",
    "            layer_ins = [node_to_layer[rgn_in] for rgn_in in rgn_partitioning]\n",
    "            num_in_units = list({li.num_output_units for li in layer_ins})\n",
    "            if len(num_in_units) > 1:\n",
    "                raise ValueError(\n",
    "                    \"Cannot build a Tucker layer, as the inputs would have different units\"\n",
    "                )\n",
    "            num_units = num_sum_units if self.region_outputs(rgn) else num_classes\n",
    "            kronecker = KroneckerLayer(num_in_units[0], arity=len(rgn_partitioning))\n",
    "            dense = SumLayer(\n",
    "                kronecker.num_output_units,\n",
    "                num_units,\n",
    "                weight_factory=sum_weight_factory,\n",
    "            )\n",
    "            layers.append(kronecker)\n",
    "            layers.append(dense)\n",
    "            in_layers[kronecker] = layer_ins\n",
    "            in_layers[dense] = [kronecker]\n",
    "            node_to_layer[rgn] = dense\n",
    "            return dense\n",
    "\n",
    "        # Set the sum-product layer builder, if necessary\n",
    "        sum_prod_builder_: Callable[[RegionNode, Sequence[RegionNode]], Layer] | None\n",
    "        if sum_product is None:\n",
    "            sum_prod_builder_ = None\n",
    "        elif sum_product == \"cp\":\n",
    "            sum_prod_builder_ = build_cp_\n",
    "        elif sum_product == \"cp-t\":\n",
    "            sum_prod_builder_ = build_cp_transposed_\n",
    "        elif sum_product == \"tucker\":\n",
    "            sum_prod_builder_ = build_tucker_\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unknown sum-product layer abstraction called {sum_product}\")\n",
    "\n",
    "        # Loop through the region graph nodes, which are already sorted in a topological ordering\n",
    "        for node in self.topological_ordering():\n",
    "            if isinstance(node, PartitionNode):  # Partition node\n",
    "                # If a sum-product layer abstraction has been specified,\n",
    "                # then just skip partition nodes\n",
    "                if sum_prod_builder_ is not None:\n",
    "                    continue\n",
    "                assert prod_factory is not None\n",
    "                partition_inputs = self.partition_inputs(node)\n",
    "                prod_inputs = [node_to_layer[rgn] for rgn in partition_inputs]\n",
    "                prod_sl = prod_factory(num_sum_units, len(prod_inputs))\n",
    "                layers.append(prod_sl)\n",
    "                in_layers[prod_sl] = prod_inputs\n",
    "                node_to_layer[node] = prod_sl\n",
    "            assert isinstance(\n",
    "                node, RegionNode\n",
    "            ), \"Region graph nodes must be either region or partition nodes\"\n",
    "            region_inputs = self.region_inputs(node)\n",
    "            region_outputs = self.region_outputs(node)\n",
    "            if not region_inputs:\n",
    "                # Input region node\n",
    "                if factorize_multivariate and len(node.scope) > 1:\n",
    "                    factorized_input_sls = [\n",
    "                        input_factory(Scope([sc]), num_input_units) for sc in node.scope\n",
    "                    ]\n",
    "                    input_sl = HadamardLayer(num_input_units, arity=len(factorized_input_sls))\n",
    "                    layers.extend(factorized_input_sls)\n",
    "                    in_layers[input_sl] = factorized_input_sls\n",
    "                else:\n",
    "                    input_sl = input_factory(node.scope, num_input_units)\n",
    "                num_units = num_sum_units if self.region_outputs(node) else num_classes\n",
    "                if sum_factory is None:\n",
    "                    layers.append(input_sl)\n",
    "                    node_to_layer[node] = input_sl\n",
    "                    continue\n",
    "                sum_sl = sum_factory(num_input_units, num_units)\n",
    "                layers.append(input_sl)\n",
    "                layers.append(sum_sl)\n",
    "                in_layers[sum_sl] = [input_sl]\n",
    "                node_to_layer[node] = sum_sl\n",
    "            elif len(region_inputs) == 1:\n",
    "                # Region node that is partitioned into one and only one way\n",
    "                (ptn,) = region_inputs\n",
    "                if sum_prod_builder_ is not None:\n",
    "                    sum_prod_builder_(node, self.partition_inputs(ptn))\n",
    "                    continue\n",
    "                num_units = num_sum_units if region_outputs else num_classes\n",
    "                sum_input = node_to_layer[ptn]\n",
    "                sum_sl = sum_factory(sum_input.num_output_units, num_units)\n",
    "                layers.append(sum_sl)\n",
    "                in_layers[sum_sl] = [sum_input]\n",
    "                node_to_layer[node] = sum_sl\n",
    "            else:  # len(node_inputs) > 1:\n",
    "                # Region node with multiple partitionings\n",
    "                num_units = num_sum_units if region_outputs else num_classes\n",
    "                if sum_prod_builder_ is None:\n",
    "                    sum_ins = [node_to_layer[ptn] for ptn in region_inputs]\n",
    "                    mix_ins = [sum_factory(sli.num_output_units, num_units) for sli in sum_ins]\n",
    "                    layers.extend(mix_ins)\n",
    "                    for mix_sl, sli in zip(mix_ins, sum_ins):\n",
    "                        in_layers[mix_sl] = [sli]\n",
    "                else:\n",
    "                    mix_ins = [\n",
    "                        sum_prod_builder_(node, self.partition_inputs(cast(PartitionNode, ptn)))\n",
    "                        for ptn in region_inputs\n",
    "                    ]\n",
    "                mix_sl = SumLayer(\n",
    "                    num_units,\n",
    "                    num_units,\n",
    "                    arity=len(mix_ins),\n",
    "                    weight_factory=nary_sum_weight_factory,\n",
    "                )\n",
    "                layers.append(mix_sl)\n",
    "                in_layers[mix_sl] = mix_ins\n",
    "                node_to_layer[node] = mix_sl\n",
    "\n",
    "        outputs = [node_to_layer[rgn] for rgn in self.outputs]\n",
    "        print(layers,\"---------------\\n\\n\\n\",in_layers,\"---------------\\n\\n\\n\",outputs)\n",
    "        return Circuit(layers, in_layers, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "207a1c94-c463-4242-8506-eca0fe748db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_circuit(rg: RegionGraph,\n",
    "                   num_input_units: int,\n",
    "                   num_sum_units: int,\n",
    "                   sum_prod_layer: str = \"cp\") -> Circuit:\n",
    "    \"\"\"Return an over-parameterized Circuit given a region-graph.\"\"\"\n",
    "    # Input layer (Gaussian) is over-parameterized by num_input_units →\n",
    "    input_factory = lambda scope, _: GaussianLayer(scope=scope,\n",
    "                                                   num_output_units=num_input_units)\n",
    "\n",
    "    # Sum-layer parameterization (softmax-normal initialisation)\n",
    "    sum_param   = Parameterization(activation=\"softmax\", initialization=\"normal\")\n",
    "    sum_factory = parameterization_to_factory(sum_param)\n",
    "\n",
    "    # Special case: mixing (n-ary sum) layers\n",
    "    nary_factory = functools.partial(mixing_weight_factory,\n",
    "                                     param_factory=sum_factory)\n",
    "    rg.build_circuit = MethodType(build_circuit2, rg)   # <─ bound to rg only\n",
    "    circuit = rg.build_circuit(\n",
    "        input_factory        = input_factory,\n",
    "        sum_weight_factory   = sum_factory,\n",
    "        nary_sum_weight_factory = nary_factory,\n",
    "        num_input_units      = num_input_units,\n",
    "        num_sum_units        = num_sum_units,\n",
    "        sum_product          = sum_prod_layer,\n",
    "    )\n",
    "    return circuit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f12522-244e-479f-bd05-ca9c0abf8eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adbcaeb5-17b4-4912-b318-0391a9034ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GaussianLayer(scope=Scope({1}), num_output_units=1, config=(scope=Scope({1}), num_output_units=1)params=(mean=Parameter(shape=(1,)), stddev=Parameter(shape=(1,)))), GaussianLayer(scope=Scope({0}), num_output_units=1, config=(scope=Scope({0}), num_output_units=1)params=(mean=Parameter(shape=(1,)), stddev=Parameter(shape=(1,)))), SumLayer(num_input_units=1, num_output_units=1, arity=2, config=(num_input_units=1, num_output_units=1, arity=2), params=(weight=Parameter(shape=(1, 2)))] ---------------\n",
      "\n",
      "\n",
      " {SumLayer(num_input_units=1, num_output_units=1, arity=2, config=(num_input_units=1, num_output_units=1, arity=2), params=(weight=Parameter(shape=(1, 2))): [GaussianLayer(scope=Scope({1}), num_output_units=1, config=(scope=Scope({1}), num_output_units=1)params=(mean=Parameter(shape=(1,)), stddev=Parameter(shape=(1,)))), GaussianLayer(scope=Scope({0}), num_output_units=1, config=(scope=Scope({0}), num_output_units=1)params=(mean=Parameter(shape=(1,)), stddev=Parameter(shape=(1,))))]} ---------------\n",
      "\n",
      "\n",
      " [SumLayer(num_input_units=1, num_output_units=1, arity=2, config=(num_input_units=1, num_output_units=1, arity=2), params=(weight=Parameter(shape=(1, 2)))]\n",
      "<cirkit.symbolic.circuit.Circuit object at 0x7ffe79468ce0>\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"134pt\" height=\"122pt\"\n",
       " viewBox=\"0.00 0.00 134.00 121.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 117.5)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-117.5 130,-117.5 130,4 -4,4\"/>\n",
       "<!-- 140730933098832 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140730933098832</title>\n",
       "<polygon fill=\"#ffbd2a\" stroke=\"#ffbd2a\" points=\"54,-38.75 0,-38.75 0,0 54,0 54,-38.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.8\" font-family=\"times italic bold\" font-size=\"21.00\" fill=\"white\">1</text>\n",
       "</g>\n",
       "<!-- 140730933087312 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140730933087312</title>\n",
       "<polygon fill=\"#607d8b\" stroke=\"#607d8b\" points=\"90,-113.5 36,-113.5 36,-74.75 90,-74.75 90,-113.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"63\" y=\"-89.55\" font-family=\"times italic bold\" font-size=\"21.00\" fill=\"white\">+</text>\n",
       "</g>\n",
       "<!-- 140730933098832&#45;&gt;140730933087312 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140730933098832&#45;&gt;140730933087312</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M36.27,-39.11C40.05,-46.75 44.51,-55.75 48.69,-64.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"45.52,-65.69 53.09,-73.1 51.79,-62.58 45.52,-65.69\"/>\n",
       "</g>\n",
       "<!-- 140730933088896 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140730933088896</title>\n",
       "<polygon fill=\"#ffbd2a\" stroke=\"#ffbd2a\" points=\"126,-38.75 72,-38.75 72,0 126,0 126,-38.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-14.8\" font-family=\"times italic bold\" font-size=\"21.00\" fill=\"white\">0</text>\n",
       "</g>\n",
       "<!-- 140730933088896&#45;&gt;140730933087312 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140730933088896&#45;&gt;140730933087312</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M89.73,-39.11C85.95,-46.75 81.49,-55.75 77.31,-64.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"74.21,-62.58 72.91,-73.1 80.48,-65.69 74.21,-62.58\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7ffea3e7cd10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from types import MethodType\n",
    "from cirkit.symbolic.io import plot_circuit\n",
    "from cirkit.templates.region_graph.graph import PartitionNode,RegionNode\n",
    "rg = RandomBinaryTree(2, depth=None, num_repetitions=1,seed=42)\n",
    "net = define_circuit(rg,\n",
    "                     num_input_units = 1,\n",
    "                     num_sum_units   = 1,\n",
    "                     sum_prod_layer  = \"cp\")\n",
    "print(net)\n",
    "from cirkit.symbolic.io import plot_circuit\n",
    "if plot_circuit is not None:\n",
    "    dot = plot_circuit(net, orientation=\"vertical\")\n",
    "    display(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c5abd67-b703-4d6a-b1dc-e2f301d70634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum-product\n",
      "lse-sum\n",
      "complex-lse-sum\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cirkit.backend.torch.semiring import SemiringImpl,SumProductSemiring\n",
    "[print(x) for x in SemiringImpl.list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30c83bb1-8aba-40d3-84d8-0f2011a2bd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.)\n",
      "tensor([[[6.]]])\n"
     ]
    }
   ],
   "source": [
    "probs = torch.tensor([0.2, 0.3, 0.5,7.0])\n",
    "result = SumProductSemiring.sum(probs)  # result = 1.0\n",
    "print(result)\n",
    "x = torch.tensor([[[1.0, 2.0]]])           # shape (1, 1, 2) = (F, B, I)\n",
    "weight = torch.tensor([[[4, 1.0]]])\n",
    "\n",
    "# Apply semiring einsum\n",
    "result = SumProductSemiring.einsum(\n",
    "    \"fbi,foi->fbo\", inputs=(x,), operands=(weight,), dim=-1, keepdim=True\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4a63f0c-9d71-4b1e-a339-c0df41047d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchCircuit(\n",
      "  (0): TorchGaussianLayer(\n",
      "    folds: 1  variables: 1  output-units: 1\n",
      "    input-shape: (1, 1, -1, 1)\n",
      "    output-shape: (1, -1, 1)\n",
      "    (mean): TorchParameter(\n",
      "      shape: (1, 1)\n",
      "      (0): TorchTensorParameter(output-shape: (1, 1))\n",
      "    )\n",
      "    (stddev): TorchParameter(\n",
      "      shape: (1, 1)\n",
      "      (0): TorchTensorParameter(output-shape: (1, 1))\n",
      "      (1): TorchScaledSigmoidParameter(\n",
      "        input-shapes: [(1, 1)]\n",
      "        output-shape: (1, 1)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): TorchGaussianLayer(\n",
      "    folds: 1  variables: 1  output-units: 1\n",
      "    input-shape: (1, 1, -1, 1)\n",
      "    output-shape: (1, -1, 1)\n",
      "    (mean): TorchParameter(\n",
      "      shape: (1, 1)\n",
      "      (0): TorchTensorParameter(output-shape: (1, 1))\n",
      "    )\n",
      "    (stddev): TorchParameter(\n",
      "      shape: (1, 1)\n",
      "      (0): TorchTensorParameter(output-shape: (1, 1))\n",
      "      (1): TorchScaledSigmoidParameter(\n",
      "        input-shapes: [(1, 1)]\n",
      "        output-shape: (1, 1)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): TorchSumLayer(\n",
      "    folds: 1  arity: 2  input-units: 1  output-units: 1\n",
      "    input-shape: (1, 2, -1, 1)\n",
      "    output-shape: (1, -1, 1)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (1, 1, 2)\n",
      "      (0): TorchTensorParameter(output-shape: (1, 1, 2))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(1, 1, 2)]\n",
      "        output-shape: (1, 1, 2)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "-------\n",
      "_nodes.0.mean._nodes.0._ptensor shape=(1, 1)\n",
      "tensor([[0.3367]])\n",
      "\n",
      "_nodes.0.stddev._nodes.0._ptensor shape=(1, 1)\n",
      "tensor([[0.1288]])\n",
      "\n",
      "_nodes.1.mean._nodes.0._ptensor shape=(1, 1)\n",
      "tensor([[0.2345]])\n",
      "\n",
      "_nodes.1.stddev._nodes.0._ptensor shape=(1, 1)\n",
      "tensor([[0.2303]])\n",
      "\n",
      "_nodes.2.weight._nodes.0._ptensor shape=(1, 1, 2)\n",
      "tensor([[[-1.1229, -0.1863]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "ctx = PipelineContext(\n",
    "    backend='torch',      # Use the PyTorch backend\n",
    "    # Specify the backend compilation flags next\n",
    "    #semiring='lse-sum',   # Use the 'lse-sum' semiring\n",
    "    semiring='sum-product', \n",
    "    fold=False,            # Enable circuit folding\n",
    "    # -------- Enable layer optimizations -------- #\n",
    "    optimize=False, # NOTE: THIS IS AN IMPORTANT FLAG (false for kronecker true for einsum)\n",
    "    # -------------------------------------------- #\n",
    ")\n",
    "cc = ctx.compile(net).to(torch.device(\"cpu\"))\n",
    "print(cc)\n",
    "# 'net' is your TorchCircuit instance\n",
    "print(\"-------\")\n",
    "for name, param in cc.named_parameters():\n",
    "    print(f\"{name:30s} shape={tuple(param.shape)}\")\n",
    "    print(param.data)   # the raw tensor\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b9b7c43-5bc1-42a5-825b-05fba8c95303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5561]],\n",
       "\n",
       "        [[0.6435]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.eval()\n",
    "test = torch.tensor([[0.5,1],[0,0]])\n",
    "print(test.shape)\n",
    "cc(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3667b47-9bd6-4b6c-9877-8effff71f926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3367]], grad_fn=<IndexBackward0>)\n",
      "tensor([0.5322], grad_fn=<SelectBackward0>)\n",
      "----\n",
      "tensor([[[[0.3447]]]], grad_fn=<ExpBackward0>) tensor([[[[0.6390]]]], grad_fn=<ExpBackward0>)\n",
      "input-shape tensor([[[[0.3447]],\n",
      "\n",
      "         [[0.6390]]]])\n",
      "tensor([[[0.5561]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from cirkit.backend.torch.layers.input import TorchGaussianLayer\n",
    "from cirkit.backend.torch.layers.inner import TorchSumLayer\n",
    "# 1) Grab *all* sub-modules…\n",
    "all_submodules = list(cc.modules())\n",
    "\n",
    "# 2) Filter out just the Gaussians and the Sum layer\n",
    "layers = [\n",
    "    m for m in all_submodules\n",
    "    if isinstance(m, (TorchGaussianLayer, TorchSumLayer))\n",
    "]\n",
    "\n",
    "# 3) Check you got three\n",
    "#print(layers)   # ->\n",
    "g0, g1, sum_layer = layers\n",
    "# Just call the parameter object directly:\n",
    "print(g0.mean())  # Or: your_layer.mean[0].forward()\n",
    "print(g0.stddev()[0])\n",
    "print(\"----\")\n",
    "\n",
    "\n",
    "x0 = torch.reshape(torch.tensor([0.5]),[1,1,-1,1])\n",
    "x1 = torch.reshape(torch.tensor([1.0]),[1,1,-1,1])\n",
    "x1, x0 = x0, x1\n",
    "l0 = g0(x0)     # same as g0.forward(x0)\n",
    "l1 = g1(x1)\n",
    "print(l0,l1)\n",
    "\n",
    "input_shape = torch.reshape(torch.tensor([[[[l0,l1]]]]),[1,2,-1,1])\n",
    "print(\"input-shape\",input_shape)\n",
    "out = sum_layer(input_shape)\n",
    "print(out)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4ab48b5-68d8-4607-b47c-5c354a8128d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.3367]], requires_grad=True) Parameter containing:\n",
      "tensor([[0.1288]], requires_grad=True) tensor([[0.5322]], grad_fn=<AddBackward0>)\n",
      "----\n",
      "Parameter containing:\n",
      "tensor([[0.2345]], requires_grad=True) Parameter containing:\n",
      "tensor([[0.2303]], requires_grad=True) tensor([[0.5573]], grad_fn=<AddBackward0>)\n",
      "----\n",
      "tensor([[0.7152]], grad_fn=<ExpBackward0>)\n",
      "tensor([[0.2787]], grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# 1) Extract the transformed parameters:\n",
    "mu0    = g0.mean._nodes[0]._ptensor          # e.g. tensor([[0.3367]])\n",
    "raw_s0 = g0.stddev._nodes[0]._ptensor        # raw\n",
    "s0     = g0.stddev._nodes[1].forward(raw_s0) # after sigmoid\n",
    "\n",
    "\n",
    "mu1    = g1.mean._nodes[0]._ptensor          # e.g. tensor([[0.1288]])\n",
    "raw_s1 = g1.stddev._nodes[0]._ptensor\n",
    "s1     = g1.stddev._nodes[1].forward(raw_s1)\n",
    "\n",
    "\n",
    "# 2) Compute the PDFs\n",
    "x0 = torch.tensor([0.5])\n",
    "x1 = torch.tensor([1.0])\n",
    "\n",
    "pdf0 = Normal(mu0, s0).log_prob(x0).exp()\n",
    "pdf1 = Normal(mu1, s1).log_prob(x1).exp()\n",
    "print(mu0,raw_s0,s0)\n",
    "print(\"----\")\n",
    "print(mu1,raw_s1,s1)\n",
    "print(\"----\")\n",
    "print(pdf0)  # ≃ tensor([[[[0.7152]]]])\n",
    "print(pdf1)  # ≃ tensor([[[[0.2787]]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d8e5bbd-f3e6-4713-960f-2219bfeb56ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28158763807102954 0.7184123619289705\n",
      "tensor([[[[0.5561]]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "p11=0.7152\n",
    "p22=0.2787\n",
    "p1=1.3866\n",
    "p2=0.0069\n",
    "w1=-1.1229\n",
    "w2=-0.1863\n",
    "#softmax\n",
    "a1 = math.exp(w1)/(math.exp(w1)+math.exp(w2))\n",
    "a2= math.exp(w2)/(math.exp(w1)+math.exp(w2))\n",
    "print(a1,a2)\n",
    "print(a1*l0+a2*l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4abb088a-bd2d-410c-b1c3-3da7e10ed94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2816]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[0.7184]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[1.]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(sum_layer(torch.tensor([[[[1.0,0.0]]]])))\n",
    "print(sum_layer(torch.tensor([[[[0.0,1.0]]]])))\n",
    "print(sum_layer(torch.tensor([[[[1.0,1.0]]]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fd36f7-719c-4ca4-9fd2-0b10e2aacbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
