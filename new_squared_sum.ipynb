{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a74976-f5b2-40d0-a847-3ce4be5bf07b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "from cirkit.symbolic.circuit import Circuit\n",
    "from cirkit.symbolic.layers import GaussianLayer, SumLayer\n",
    "\n",
    "def build_circuit_one_sum(\n",
    "    self,\n",
    "    *,\n",
    "    input_factory,        # like lambda scope, n: GaussianLayer(scope, n)\n",
    "    sum_weight_factory,   # your ParameterFactory for the SumLayer\n",
    "    num_input_units: int, # # of outputs per Gaussian leaf\n",
    "    num_sum_units: int,   # # of mixtures in the one SumLayer\n",
    ") -> Circuit:\n",
    "    # 1) Find all the leaves in the region graph:\n",
    "    leaves = [node for node in self.topological_ordering()\n",
    "              if not self.region_inputs(node)]\n",
    "    \n",
    "    layers = []\n",
    "    in_layers = {}\n",
    "    \n",
    "    # 2) Build one GaussianLayer per leaf\n",
    "    gaussians = []\n",
    "    for leaf in leaves:\n",
    "        gauss = input_factory(leaf.scope, num_input_units)\n",
    "        layers.append(gauss)\n",
    "        gaussians.append(gauss)\n",
    "    \n",
    "    # 3) Build *one* SumLayer mixing them all\n",
    "    sum_layer = SumLayer(\n",
    "        num_input_units=num_input_units,\n",
    "        num_output_units=num_sum_units,\n",
    "        arity=len(gaussians),\n",
    "        weight_factory=sum_weight_factory,\n",
    "    )\n",
    "    layers.append(sum_layer)\n",
    "    in_layers[sum_layer] = gaussians\n",
    "    \n",
    "    # 4) Return a circuit whose only output is that top‐sum\n",
    "    print(layers,\"---------------\\n\\n\\n\",in_layers,\"---------------\\n\\n\\n\",[sum_layer])\n",
    "    return Circuit(layers, in_layers, outputs=[sum_layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db0999c-d30b-4238-ab17-fe382230cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from types import MethodType\n",
    "from cirkit.templates.region_graph import RandomBinaryTree\n",
    "from cirkit.templates.utils import Parameterization, parameterization_to_factory\n",
    "from cirkit.symbolic.parameters import mixing_weight_factory\n",
    "\n",
    "def define_circuit_one_sum(\n",
    "    num_input_units: int = 3,\n",
    "    num_sum_units:   int = 2,\n",
    ") -> Circuit:\n",
    "    # ── 1) Build the region‐graph (just to get leaf scopes) ────────────\n",
    "    rg = RandomBinaryTree(1, depth=None, num_repetitions=1, seed=42)\n",
    "    \n",
    "    # ── 2) Attach our star‐builder ────────────────────────────────────\n",
    "    rg.build_circuit = MethodType(build_circuit_one_sum, rg)\n",
    "    \n",
    "    # ── 3) Make the factories ─────────────────────────────────────────\n",
    "    input_factory = lambda scope, n: GaussianLayer(scope=scope, num_output_units=n)\n",
    "    p = Parameterization(activation=\"softmax\", initialization=\"normal\")\n",
    "    sum_param_factory = parameterization_to_factory(p)\n",
    "    # (we don’t need an n‐ary mixing factory here, just the base factory)\n",
    "    \n",
    "    # ── 4) Build & return ─────────────────────────────────────────────\n",
    "    return rg.build_circuit(\n",
    "        input_factory=input_factory,\n",
    "        sum_weight_factory=sum_param_factory,\n",
    "        num_input_units=num_input_units,\n",
    "        num_sum_units=num_sum_units,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d282d26e-cfdc-48d0-a0c9-33073e877db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GaussianLayer(scope=Scope({0}), num_output_units=100, config=(scope=Scope({0}), num_output_units=100)params=(mean=Parameter(shape=(100,)), stddev=Parameter(shape=(100,)))), SumLayer(num_input_units=100, num_output_units=100, arity=1, config=(num_input_units=100, num_output_units=100, arity=1), params=(weight=Parameter(shape=(100, 100)))] ---------------\n",
      "\n",
      "\n",
      " {SumLayer(num_input_units=100, num_output_units=100, arity=1, config=(num_input_units=100, num_output_units=100, arity=1), params=(weight=Parameter(shape=(100, 100))): [GaussianLayer(scope=Scope({0}), num_output_units=100, config=(scope=Scope({0}), num_output_units=100)params=(mean=Parameter(shape=(100,)), stddev=Parameter(shape=(100,))))]} ---------------\n",
      "\n",
      "\n",
      " [SumLayer(num_input_units=100, num_output_units=100, arity=1, config=(num_input_units=100, num_output_units=100, arity=1), params=(weight=Parameter(shape=(100, 100)))]\n",
      "Structural properties:\n",
      "  - Smoothness: True\n",
      "  - Decomposability: True\n",
      "  - Structured-decomposability: True\n",
      "TorchCircuit(\n",
      "  (0): TorchGaussianLayer(\n",
      "    folds: 1  variables: 1  output-units: 100\n",
      "    input-shape: (1, 1, -1, 1)\n",
      "    output-shape: (1, -1, 100)\n",
      "    (mean): TorchParameter(\n",
      "      shape: (1, 100)\n",
      "      (0): TorchTensorParameter(output-shape: (1, 100))\n",
      "    )\n",
      "    (stddev): TorchParameter(\n",
      "      shape: (1, 100)\n",
      "      (0): TorchTensorParameter(output-shape: (1, 100))\n",
      "      (1): TorchScaledSigmoidParameter(\n",
      "        input-shapes: [(1, 100)]\n",
      "        output-shape: (1, 100)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): TorchSumLayer(\n",
      "    folds: 1  arity: 1  input-units: 100  output-units: 100\n",
      "    input-shape: (1, 1, -1, 100)\n",
      "    output-shape: (1, -1, 100)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (1, 100, 100)\n",
      "      (0): TorchTensorParameter(output-shape: (1, 100, 100))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(1, 100, 100)]\n",
      "        output-shape: (1, 100, 100)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"62pt\" height=\"122pt\"\n",
       " viewBox=\"0.00 0.00 62.00 121.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 117.5)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-117.5 58,-117.5 58,4 -4,4\"/>\n",
       "<!-- 140731043463968 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140731043463968</title>\n",
       "<polygon fill=\"#ffbd2a\" stroke=\"#ffbd2a\" points=\"54,-38.75 0,-38.75 0,0 54,0 54,-38.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.8\" font-family=\"times italic bold\" font-size=\"21.00\" fill=\"white\">0</text>\n",
       "</g>\n",
       "<!-- 140736548497136 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140736548497136</title>\n",
       "<polygon fill=\"#607d8b\" stroke=\"#607d8b\" points=\"54,-113.5 0,-113.5 0,-74.75 54,-74.75 54,-113.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-89.55\" font-family=\"times italic bold\" font-size=\"21.00\" fill=\"white\">+</text>\n",
       "</g>\n",
       "<!-- 140731043463968&#45;&gt;140736548497136 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140731043463968&#45;&gt;140736548497136</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M27,-39.11C27,-46.41 27,-54.96 27,-63.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"23.5,-62.94 27,-72.94 30.5,-62.94 23.5,-62.94\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7ffea9d6af00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tweak these as you like:\n",
    "NUM_INPUT_UNITS = 100 # e.g. 4 Gaussians per leaf\n",
    "NUM_SUM_UNITS   = 100   # e.g. 5 mixture components in the single SumLayer\n",
    "\n",
    "net = define_circuit_one_sum(NUM_INPUT_UNITS, NUM_SUM_UNITS)\n",
    "\n",
    "from cirkit.pipeline import PipelineContext\n",
    "ctx = PipelineContext(backend=\"torch\", semiring=\"sum-product\", fold=False, optimize=False)\n",
    "cc  = ctx.compile(net).cpu().eval()\n",
    "# Print which structural properties the circuit satisfies\n",
    "print(f'Structural properties:')\n",
    "print(f'  - Smoothness: {net.is_smooth}')\n",
    "print(f'  - Decomposability: {net.is_decomposable}')\n",
    "print(f'  - Structured-decomposability: {net.is_structured_decomposable}')\n",
    "print(cc)\n",
    "from cirkit.symbolic.io import plot_circuit\n",
    "if plot_circuit is not None:\n",
    "    dot = plot_circuit(net, orientation=\"vertical\")\n",
    "    display(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "517dfcbb-19f1-41bb-a720-70cf364e35a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 100]) torch.Size([3, 100])\n",
      "✅ Manual calculation matches the circuit output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# --- assume cc is your TorchCircuit instance ---\n",
    "# e.g. cc = TorchCircuit(...)\n",
    "\n",
    "# 1) Prepare inputs\n",
    "input_vals = torch.tensor([[0.5], [1.0], [0.3]])  # shape (batch=3, vars=1)\n",
    "\n",
    "# 2) Run the circuit\n",
    "net_out = cc(input_vals)  # shape (3, 100)\n",
    "net_out = net_out.squeeze(1) \n",
    "# 3) Extract the GaussianLayer and compute per-component PDFs\n",
    "gi = cc.layers[0]  # TorchGaussianLayer\n",
    "\n",
    "# raw parameters for mu and sigma\n",
    "mu_node      = gi.mean._nodes[0]._ptensor            # shape (1, 200)\n",
    "raw_sigma    = gi.stddev._nodes[0]._ptensor          # shape (1, 200)\n",
    "sigma        = gi.stddev._nodes[1].forward(raw_sigma)# after sigmoid, shape (1, 200)\n",
    "\n",
    "# expand to batch-size\n",
    "# mu & sigma: (1,200) -> (3,200)\n",
    "mu_exp   = mu_node.expand(input_vals.size(0), -1)\n",
    "sigma_exp= sigma.expand(input_vals.size(0), -1)\n",
    "\n",
    "# now compute each PDF for each output-unit i\n",
    "# input_vals[:,0] is shape (3,), unsqueeze to (3,1) to broadcast against (3,200)\n",
    "x = input_vals[:, 0].unsqueeze(1)  # (3,1)\n",
    "normal = Normal(mu_exp, sigma_exp)\n",
    "g_manual = normal.log_prob(x).exp()  # (3,200)\n",
    "\n",
    "# 4) Extract the SumLayer weights and normalize\n",
    "sl = cc.layers[1]  # TorchSumLayer\n",
    "raw_w   = sl.weight._nodes[0]._ptensor       # shape (1,100,200)\n",
    "w_norm  = sl.weight._nodes[1].forward(raw_w) # softmaxed along last dim, shape (1,100,200)\n",
    "w_norm  = w_norm.squeeze(0)                  # (100,200)\n",
    "\n",
    "# 5) Manually do the weighted sum: for each sample b and each output-unit j\n",
    "#    y_manual[b,j] = sum_i g_manual[b,i] * w_norm[j,i]\n",
    "#    g_manual: (3,200) -> (3,1,200), w_norm: (100,200) -> (1,100,200)\n",
    "y_manual = (g_manual.unsqueeze(1) * w_norm.unsqueeze(0)).sum(-1)  # (3,100)\n",
    "print(net_out.shape,y_manual.shape)\n",
    "# print(net_out,y_manual)\n",
    "# 6) Assert they match\n",
    "assert torch.allclose(net_out, y_manual, atol=1e-6), \"Mismatch between network and manual!\"\n",
    "print(\"✅ Manual calculation matches the circuit output.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "890de071-b800-4148-897b-b9fd123cf3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<cirkit.symbolic.circuit.Circuit object at 0x7ffe7ec61ca0>\n",
      "TorchCircuit(\n",
      "  (0): TorchGaussianLayer(\n",
      "    folds: 1  variables: 1  output-units: 10000\n",
      "    input-shape: (1, 1, -1, 1)\n",
      "    output-shape: (1, -1, 10000)\n",
      "    (mean): TorchParameter(\n",
      "      shape: (1, 10000)\n",
      "      (0): TorchPointerParameter(\n",
      "        output-shape: (1, 100)\n",
      "        (_parameter): TorchTensorParameter(output-shape: (1, 100))\n",
      "      )\n",
      "      (1): TorchPointerParameter(\n",
      "        output-shape: (1, 100)\n",
      "        (_parameter): TorchTensorParameter(output-shape: (1, 100))\n",
      "      )\n",
      "      (2): TorchPointerParameter(\n",
      "        output-shape: (1, 100)\n",
      "        (_parameter): TorchTensorParameter(output-shape: (1, 100))\n",
      "      )\n",
      "      (3): TorchPointerParameter(\n",
      "        output-shape: (1, 100)\n",
      "        (_parameter): TorchTensorParameter(output-shape: (1, 100))\n",
      "      )\n",
      "      (4): TorchScaledSigmoidParameter(\n",
      "        input-shapes: [(1, 100)]\n",
      "        output-shape: (1, 100)\n",
      "      )\n",
      "      (5): TorchScaledSigmoidParameter(\n",
      "        input-shapes: [(1, 100)]\n",
      "        output-shape: (1, 100)\n",
      "      )\n",
      "      (6): TorchGaussianProductMean(\n",
      "        input-shapes: [(1, 100), (1, 100), (1, 100), (1, 100)]\n",
      "        output-shape: (1, 10000)\n",
      "      )\n",
      "    )\n",
      "    (stddev): TorchParameter(\n",
      "      shape: (1, 10000)\n",
      "      (0): TorchPointerParameter(\n",
      "        output-shape: (1, 100)\n",
      "        (_parameter): TorchTensorParameter(output-shape: (1, 100))\n",
      "      )\n",
      "      (1): TorchPointerParameter(\n",
      "        output-shape: (1, 100)\n",
      "        (_parameter): TorchTensorParameter(output-shape: (1, 100))\n",
      "      )\n",
      "      (2): TorchScaledSigmoidParameter(\n",
      "        input-shapes: [(1, 100)]\n",
      "        output-shape: (1, 100)\n",
      "      )\n",
      "      (3): TorchScaledSigmoidParameter(\n",
      "        input-shapes: [(1, 100)]\n",
      "        output-shape: (1, 100)\n",
      "      )\n",
      "      (4): TorchGaussianProductStddev(\n",
      "        input-shapes: [(1, 100), (1, 100)]\n",
      "        output-shape: (1, 10000)\n",
      "      )\n",
      "    )\n",
      "    (log_partition): TorchParameter(\n",
      "      shape: (1, 10000)\n",
      "      (0): TorchPointerParameter(\n",
      "        output-shape: (1, 100)\n",
      "        (_parameter): TorchTensorParameter(output-shape: (1, 100))\n",
      "      )\n",
      "      (1): TorchPointerParameter(\n",
      "        output-shape: (1, 100)\n",
      "        (_parameter): TorchTensorParameter(output-shape: (1, 100))\n",
      "      )\n",
      "      (2): TorchPointerParameter(\n",
      "        output-shape: (1, 100)\n",
      "        (_parameter): TorchTensorParameter(output-shape: (1, 100))\n",
      "      )\n",
      "      (3): TorchPointerParameter(\n",
      "        output-shape: (1, 100)\n",
      "        (_parameter): TorchTensorParameter(output-shape: (1, 100))\n",
      "      )\n",
      "      (4): TorchScaledSigmoidParameter(\n",
      "        input-shapes: [(1, 100)]\n",
      "        output-shape: (1, 100)\n",
      "      )\n",
      "      (5): TorchScaledSigmoidParameter(\n",
      "        input-shapes: [(1, 100)]\n",
      "        output-shape: (1, 100)\n",
      "      )\n",
      "      (6): TorchGaussianProductLogPartition(\n",
      "        input-shapes: [(1, 100), (1, 100), (1, 100), (1, 100)]\n",
      "        output-shape: (1, 10000)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): TorchSumLayer(\n",
      "    folds: 1  arity: 1  input-units: 10000  output-units: 10000\n",
      "    input-shape: (1, 1, -1, 10000)\n",
      "    output-shape: (1, -1, 10000)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (1, 10000, 10000)\n",
      "      (0): TorchPointerParameter(\n",
      "        output-shape: (1, 100, 100)\n",
      "        (_parameter): TorchTensorParameter(output-shape: (1, 100, 100))\n",
      "      )\n",
      "      (1): TorchPointerParameter(\n",
      "        output-shape: (1, 100, 100)\n",
      "        (_parameter): TorchTensorParameter(output-shape: (1, 100, 100))\n",
      "      )\n",
      "      (2): TorchSoftmaxParameter(\n",
      "        input-shapes: [(1, 100, 100)]\n",
      "        output-shape: (1, 100, 100)\n",
      "      )\n",
      "      (3): TorchSoftmaxParameter(\n",
      "        input-shapes: [(1, 100, 100)]\n",
      "        output-shape: (1, 100, 100)\n",
      "      )\n",
      "      (4): TorchKroneckerParameter(\n",
      "        input-shapes: [(1, 100, 100), (1, 100, 100)]\n",
      "        output-shape: (1, 10000, 10000)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"62pt\" height=\"122pt\"\n",
       " viewBox=\"0.00 0.00 62.00 121.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 117.5)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-117.5 58,-117.5 58,4 -4,4\"/>\n",
       "<!-- 140731043064448 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140731043064448</title>\n",
       "<polygon fill=\"#ffbd2a\" stroke=\"#ffbd2a\" points=\"54,-38.75 0,-38.75 0,0 54,0 54,-38.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.8\" font-family=\"times italic bold\" font-size=\"21.00\" fill=\"white\">0</text>\n",
       "</g>\n",
       "<!-- 140731025332448 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140731025332448</title>\n",
       "<polygon fill=\"#607d8b\" stroke=\"#607d8b\" points=\"54,-113.5 0,-113.5 0,-74.75 54,-74.75 54,-113.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-89.55\" font-family=\"times italic bold\" font-size=\"21.00\" fill=\"white\">+</text>\n",
       "</g>\n",
       "<!-- 140731043064448&#45;&gt;140731025332448 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140731043064448&#45;&gt;140731025332448</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M27,-39.11C27,-46.41 27,-54.96 27,-63.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"23.5,-62.94 27,-72.94 30.5,-62.94 23.5,-62.94\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7ffe78d69ee0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cirkit.symbolic.functional as SF\n",
    "\n",
    "# Construct the circuit computing Z, i.e., the integral of |c(X)|^2 over the complete domain of X\n",
    "symbolic_circuit_partition_func = SF.multiply(net, net)\n",
    "print(symbolic_circuit_partition_func)\n",
    "print(ctx.compile(symbolic_circuit_partition_func))\n",
    "plot_circuit(symbolic_circuit_partition_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b8ca63b-bd4e-47c4-b8d2-a3eb2b165087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "x1: shape=(4, 2, 3), device=cuda:0, dtype=torch.float32\n",
      "x2: shape=(4, 3, 2), device=cuda:0, dtype=torch.float32\n",
      "y : shape=(4, 6, 6), device=cuda:0, dtype=torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from cirkit.backend.torch.parameters.nodes import TorchKroneckerParameter\n",
    "\n",
    "# 1 Pick the device once\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using\", device)\n",
    "\n",
    "# 2 Create the re-parameterisation module on that device\n",
    "p = TorchKroneckerParameter(\n",
    "        in_shape1=(2, 3),\n",
    "        in_shape2=(3, 2),\n",
    "        num_folds=4,\n",
    ").to(device)                     # move parameters/buffers to GPU\n",
    "\n",
    "# 3 Generate inputs on the same device\n",
    "x1 = torch.randn(4, 2, 3, device=device)\n",
    "x2 = torch.randn(4, 3, 2, device=device)\n",
    "\n",
    "# 4 Forward pass\n",
    "y = p.forward(x1, x2)\n",
    "\n",
    "# 5 Inspect shapes, devices and dtypes\n",
    "def info(label, t):\n",
    "    print(f\"{label}: shape={tuple(t.shape)}, device={t.device}, dtype={t.dtype}\")\n",
    "\n",
    "info(\"x1\", x1)\n",
    "info(\"x2\", x2)\n",
    "info(\"y \", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0009ddc-effa-47cf-8b40-53f5a6addcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda…\n",
      "\n",
      "size |    c |  kron ms |  kron MB |   nys ms |   nys MB |  rel err\n",
      "----------------------------------------------------------------\n",
      "   2 |    4 |     8.33 |     0.00 |   152.96 |     8.53 |   0.0000\n",
      "   4 |    4 |     0.14 |     8.54 |     9.54 |     8.53 |   0.6459\n",
      "   4 |   16 |     0.14 |     8.54 |    10.99 |     8.53 |   0.0000\n",
      "   8 |    4 |     0.10 |     8.86 |    17.93 |    11.69 |   0.8987\n",
      "   8 |    8 |     0.10 |     8.86 |    14.97 |    11.69 |   0.7913\n",
      "   8 |   16 |     0.10 |     8.86 |    17.94 |    11.70 |   0.6321\n",
      "   8 |   64 |     0.10 |     8.86 |    29.19 |     8.66 |   0.0000\n",
      "  16 |    4 |     0.11 |    13.80 |    15.21 |    11.94 |   0.9706\n",
      "  16 |    8 |     0.11 |    13.80 |    17.43 |    11.96 |   0.9418\n",
      "  16 |   16 |     0.11 |    13.80 |    19.01 |    11.98 |   0.8907\n",
      "  16 |   32 |     0.11 |    13.80 |    22.34 |    12.03 |   0.7905\n",
      "  16 |   64 |     0.11 |    13.80 |    41.32 |    12.15 |   0.6118\n",
      "  16 |  256 |     0.11 |    13.80 |   151.50 |    10.43 |   0.0000\n",
      "  32 |    4 |     0.46 |    92.57 |    48.05 |    21.13 |   0.9926\n",
      "  32 |    8 |     0.46 |    92.57 |    47.39 |    21.14 |   0.9849\n",
      "  32 |   16 |     0.46 |    92.57 |    50.31 |    21.18 |   0.9698\n",
      "  32 |   32 |     0.46 |    92.57 |    50.63 |    21.24 |   0.9396\n",
      "  32 |   64 |     0.46 |    92.57 |    61.28 |    21.37 |   0.8851\n",
      "  32 |  128 |     0.46 |    92.57 |    89.46 |    21.64 |   0.7837\n",
      "  32 |  180 |     0.46 |    92.57 |   139.62 |    21.85 |   0.7081\n",
      "  32 | 1024 |     0.46 |    92.57 |  1313.57 |    38.18 |   0.0000\n",
      "  64 |    4 |     3.77 |  1351.35 |  1099.83 |   209.94 |   0.9981\n",
      "  64 |    8 |     3.77 |  1351.35 |  1087.63 |   210.01 |   0.9961\n",
      "  64 |   16 |     3.77 |  1351.35 |  1090.18 |   210.14 |   0.9923\n",
      "  64 |   32 |     3.77 |  1351.35 |  1091.94 |   210.40 |   0.9848\n",
      "  64 |   64 |     3.77 |  1351.35 |  1120.85 |   210.93 |   0.9700\n",
      "  64 |  128 |     3.77 |  1351.35 |  1164.86 |   211.98 |   0.9418\n",
      "  64 |  180 |     3.77 |  1351.35 |  1179.22 |   212.83 |   0.9188\n",
      "  64 | 4096 |     3.77 |  1351.35 | 93602.96 |   480.43 |   0.0001\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 GiB. GPU 0 has a total capacity of 11.59 GiB of which 10.24 GiB is free. Of the allocated memory 10.62 MiB is allocated by PyTorch, and 9.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m    105\u001b[39m x2_cpu = torch.randn(folds, s, s, dtype=dtype)\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# Kronecker\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m y_full_cpu, kron_ms, kron_mb = \u001b[43mkron_time_mem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1_cpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2_cpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m _, n = y_full_cpu.shape[-\u001b[32m2\u001b[39m:]                     \u001b[38;5;66;03m# width for Nyström\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Nyström sample budgets to test\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mkron_time_mem\u001b[39m\u001b[34m(p, x1_cpu, x2_cpu, device)\u001b[39m\n\u001b[32m     26\u001b[39m torch.cuda.synchronize()\n\u001b[32m     27\u001b[39m t0 = time.perf_counter()\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m y_gpu = \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m                 \u001b[38;5;66;03m# <<< the only op we measure\u001b[39;00m\n\u001b[32m     31\u001b[39m torch.cuda.synchronize()\n\u001b[32m     32\u001b[39m t_ms   = (time.perf_counter() - t0) * \u001b[32m1e3\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/0fh5bymigi244fcx5fm29fbs5ibhrvwr-msc-cirkit-dev-env/lib/python3.12/site-packages/cirkit/backend/torch/parameters/nodes.py:522\u001b[39m, in \u001b[36mTorchKroneckerParameter.forward\u001b[39m\u001b[34m(self, x1, x2)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1: Tensor, x2: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batched_kron\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/0fh5bymigi244fcx5fm29fbs5ibhrvwr-msc-cirkit-dev-env/lib/python3.12/site-packages/torch/_functorch/apis.py:203\u001b[39m, in \u001b[36mvmap.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/0fh5bymigi244fcx5fm29fbs5ibhrvwr-msc-cirkit-dev-env/lib/python3.12/site-packages/torch/_functorch/vmap.py:331\u001b[39m, in \u001b[36mvmap_impl\u001b[39m\u001b[34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[32m    321\u001b[39m         func,\n\u001b[32m    322\u001b[39m         flat_in_dims,\n\u001b[32m   (...)\u001b[39m\u001b[32m    327\u001b[39m         **kwargs,\n\u001b[32m    328\u001b[39m     )\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/0fh5bymigi244fcx5fm29fbs5ibhrvwr-msc-cirkit-dev-env/lib/python3.12/site-packages/torch/_functorch/vmap.py:479\u001b[39m, in \u001b[36m_flat_vmap\u001b[39m\u001b[34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[32m    476\u001b[39m     batched_inputs = _create_batched_inputs(\n\u001b[32m    477\u001b[39m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[32m    478\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     batched_outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 0 has a total capacity of 11.59 GiB of which 10.24 GiB is free. Of the allocated memory 10.62 MiB is allocated by PyTorch, and 9.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Experiment: Full Kronecker vs. basic Nyström approximation\n",
    "----------------------------------------------------------\n",
    "* Single tuning knob: `sizes` – list of matrix edge-lengths `s`\n",
    "  – x1 : (s × s) , x2 : (s × s)         (non-symmetric weights)\n",
    "  – y  : (s² × s²)  Kronecker product\n",
    "* We time / measure peak GPU memory ONLY for the forward pass itself.\n",
    "* After every timed section all GPU tensors are moved back to CPU\n",
    "  and the cache is cleared, preventing GPU OOM.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from cirkit.backend.torch.parameters.nodes import TorchKroneckerParameter\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# helpers\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def kron_time_mem(p, x1_cpu, x2_cpu, device):\n",
    "    \"\"\"Move inputs to GPU, run forward once, immediately free memory.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    x1 = x1_cpu.to(device, non_blocking=True)\n",
    "    x2 = x2_cpu.to(device, non_blocking=True)\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    y_gpu = p.forward(x1, x2)                 # <<< the only op we measure\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t_ms   = (time.perf_counter() - t0) * 1e3\n",
    "    mem_mb = torch.cuda.max_memory_allocated(device) / 1e6\n",
    "\n",
    "    y_cpu = y_gpu.cpu()                       # keep result for error calc\n",
    "    del y_gpu, x1, x2\n",
    "    torch.cuda.empty_cache()\n",
    "    return y_cpu, t_ms, mem_mb\n",
    "\n",
    "\n",
    "def nystrom_time_mem(A_cpu, c, device):\n",
    "    \"\"\"\n",
    "    Very basic Nyström: one fold at a time on the GPU.\n",
    "    Returns the CPU reconstruction and timing / peak-mem for the whole sweep.\n",
    "    \"\"\"\n",
    "    folds, m, n = A_cpu.shape\n",
    "    A_hat_cpu = torch.empty_like(A_cpu)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    peak_mb = 0.0\n",
    "    for f in range(folds):\n",
    "        A_f = A_cpu[f].to(device, non_blocking=True)          # GPU copy\n",
    "        idx = torch.randperm(n, device=device)[:c]\n",
    "        C   = A_f[:, idx]                                     # (m, c)\n",
    "        A_hat_f = C @ torch.linalg.pinv(C) @ A_f              # Nyström\n",
    "\n",
    "        A_hat_cpu[f] = A_hat_f.cpu()                          # back to CPU\n",
    "        peak_mb = max(peak_mb,\n",
    "                      torch.cuda.max_memory_allocated(device) / 1e6)\n",
    "\n",
    "        del A_f, C, A_hat_f\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t_ms = (time.perf_counter() - t0) * 1e3\n",
    "    return A_hat_cpu, t_ms, peak_mb\n",
    "\n",
    "\n",
    "def rel_fro_error(A, A_hat):\n",
    "    return (torch.linalg.norm(A - A_hat) /\n",
    "            torch.linalg.norm(A)).item()\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# experiment configuration\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype  = torch.float32\n",
    "folds  = 20\n",
    "\n",
    "sizes  = [2, 4, 8, 16, 32, 64, 128, 180, 200]   # matrix edge lengths s\n",
    "base_sample_counts = [4, 8, 16, 32, 64, 128, 180]\n",
    "\n",
    "print(f\"Running on {device}…\\n\")\n",
    "print(f\"{'size':>4} | {'c':>4} | \"\n",
    "      f\"{'kron ms':>8} | {'kron MB':>8} | \"\n",
    "      f\"{'nys ms':>8} | {'nys MB':>8} | \"\n",
    "      f\"{'rel err':>8}\")\n",
    "print('-' * 64)\n",
    "\n",
    "for s in sizes:\n",
    "    # Build parameter on GPU (it is tiny)\n",
    "    p = TorchKroneckerParameter(\n",
    "            in_shape1=(s, s),\n",
    "            in_shape2=(s, s),\n",
    "            num_folds=folds\n",
    "        ).to(device)\n",
    "\n",
    "    # Inputs live on CPU until the exact moment we need them on GPU\n",
    "    x1_cpu = torch.randn(folds, s, s, dtype=dtype)\n",
    "    x2_cpu = torch.randn(folds, s, s, dtype=dtype)\n",
    "\n",
    "    # Kronecker\n",
    "    y_full_cpu, kron_ms, kron_mb = kron_time_mem(p, x1_cpu, x2_cpu, device)\n",
    "    _, n = y_full_cpu.shape[-2:]                     # width for Nyström\n",
    "\n",
    "    # Nyström sample budgets to test\n",
    "    sample_counts = [c for c in base_sample_counts if c <= int(n/3)]\n",
    "    if n not in sample_counts:\n",
    "        sample_counts.append(n)                      # full rank baseline\n",
    "\n",
    "    for c in sample_counts:\n",
    "        y_hat_cpu, nys_ms, nys_mb = nystrom_time_mem(y_full_cpu, c, device)\n",
    "        err = rel_fro_error(y_full_cpu, y_hat_cpu)\n",
    "\n",
    "        print(f\"{s:>4} | {c:>4} | \"\n",
    "              f\"{kron_ms:8.2f} | {kron_mb:8.2f} | \"\n",
    "              f\"{nys_ms:8.2f} | {nys_mb:8.2f} | \"\n",
    "              f\"{err:8.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
