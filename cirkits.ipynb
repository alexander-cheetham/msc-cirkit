{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b63f493-a57a-4296-858f-0cd442f8b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.templates import data_modalities, utils\n",
    "\n",
    "symbolic_circuit = data_modalities.image_data(\n",
    "    (1, 28, 28),                # The shape of MNIST image, i.e., (num_channels, image_height, image_width)\n",
    "    region_graph='quad-graph',  # Select the structure of the circuit to follow the QuadGraph region graph\n",
    "    input_layer='categorical',  # Use Categorical distributions for the pixel values (0-255) as input layers\n",
    "    num_input_units=64,         # Each input layer consists of 64 Categorical input units\n",
    "    sum_product_layer='cp',     # Use CP sum-product layers, i.e., alternate dense layers with Hadamard product layers\n",
    "    num_sum_units=64,# Each dense sum layer consists of 64 sum units\n",
    "    num_classes=1,\n",
    "    sum_weight_param=utils.Parameterization(\n",
    "        activation='softmax',   # Parameterize the sum weights by using a softmax activation\n",
    "        initialization='normal' # Initialize the sum weights by sampling from a standard normal distribution\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf3eac8-54ed-41c7-bcd7-f4ba3f60bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables: 784\n",
      "\n",
      "Structural properties:\n",
      "  - Smoothness: True\n",
      "  - Decomposability: True\n",
      "  - Structured-decomposability: False\n"
     ]
    }
   ],
   "source": [
    "# Print some information\n",
    "print(f'Number of variables: {symbolic_circuit.num_variables}')\n",
    "print()\n",
    "\n",
    "# Print which structural properties the circuit satisfies\n",
    "print(f'Structural properties:')\n",
    "print(f'  - Smoothness: {symbolic_circuit.is_smooth}')\n",
    "print(f'  - Decomposability: {symbolic_circuit.is_decomposable}')\n",
    "print(f'  - Structured-decomposability: {symbolic_circuit.is_structured_decomposable}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b312e0da-a1d3-4880-a7bb-21237ee3130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set some seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set the torch device to use\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c84d57e-e514-4fc9-95c0-1822710a6ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.39 s, sys: 66.7 ms, total: 1.46 s\n",
      "Wall time: 1.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from cirkit.pipeline import compile\n",
    "circuit = compile(symbolic_circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24abb86d-12ef-42dc-a692-174ff3ae1ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 5725\n",
      "Number of learnable parameters: 25657730\n"
     ]
    }
   ],
   "source": [
    "# Print some statistics\n",
    "num_layers = len(list(symbolic_circuit.layers))\n",
    "print(f\"Number of layers: {num_layers}\")\n",
    "num_parameters = sum(p.numel() for p in circuit.parameters())\n",
    "print(f\"Number of learnable parameters: {num_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd584dc9-16c0-4528-801c-5b5cbbb92768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Load the MNIST data set and data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Flatten the images and set pixel values in the [0-255] range\n",
    "    transforms.Lambda(lambda x: (255 * x.view(-1)).long())\n",
    "])\n",
    "data_train = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "# Instantiate the training and testing data loaders\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, batch_size=256)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256)\n",
    "\n",
    "# Initialize a torch optimizer of your choice,\n",
    "#  e.g., Adam, by passing the parameters of the circuit\n",
    "optimizer = optim.Adam(circuit.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb548216-1b48-4187-80ed-2518deb37fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|███████████████████████████████████████████████████████████████████████████| 235/235 [00:08<00:00, 28.82it/s, Average NLL=2492.162]\n",
      "Epoch 2/10: 100%|████████████████████████████████████████████████████████████████████████████| 235/235 [00:07<00:00, 30.00it/s, Average NLL=895.922]\n",
      "Epoch 3/10: 100%|████████████████████████████████████████████████████████████████████████████| 235/235 [00:07<00:00, 30.00it/s, Average NLL=785.745]\n",
      "Epoch 4/10: 100%|████████████████████████████████████████████████████████████████████████████| 235/235 [00:07<00:00, 29.93it/s, Average NLL=749.998]\n",
      "Epoch 5/10: 100%|████████████████████████████████████████████████████████████████████████████| 235/235 [00:07<00:00, 30.04it/s, Average NLL=729.829]\n",
      "Epoch 6/10: 100%|████████████████████████████████████████████████████████████████████████████| 235/235 [00:07<00:00, 29.95it/s, Average NLL=707.061]\n",
      "Epoch 7/10: 100%|████████████████████████████████████████████████████████████████████████████| 235/235 [00:07<00:00, 30.09it/s, Average NLL=698.309]\n",
      "Epoch 8/10: 100%|████████████████████████████████████████████████████████████████████████████| 235/235 [00:07<00:00, 30.01it/s, Average NLL=693.299]\n",
      "Epoch 9/10: 100%|████████████████████████████████████████████████████████████████████████████| 235/235 [00:07<00:00, 30.07it/s, Average NLL=686.725]\n",
      "Epoch 10/10: 100%|███████████████████████████████████████████████████████████████████████████| 235/235 [00:07<00:00, 30.05it/s, Average NLL=683.997]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 10\n",
    "step_idx = 0\n",
    "running_loss = 0.0\n",
    "running_samples = 0\n",
    "\n",
    "# Move the circuit to chosen device\n",
    "circuit = circuit.to(device)\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    # Create a tqdm progress bar for the inner loop of the current epoch\n",
    "    epoch_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch_idx+1}/{num_epochs}\")\n",
    "    for i, (batch, _) in enumerate(epoch_bar):\n",
    "        # The circuit expects an input of shape (batch_dim, num_variables)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Compute the log-likelihoods of the batch by evaluating the circuit\n",
    "        log_likelihoods = circuit(batch)\n",
    "\n",
    "        # We take the negated average log-likelihood as loss\n",
    "        loss = -torch.mean(log_likelihoods)\n",
    "        loss.backward()\n",
    "        # Update the parameters of the circuit, as with any other model in PyTorch\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.detach() * len(batch)\n",
    "        running_samples += len(batch)\n",
    "        step_idx += 1\n",
    "\n",
    "        if step_idx % 200 == 0:\n",
    "            average_nll = running_loss / running_samples\n",
    "            # Update the tqdm progress bar with the latest average NLL\n",
    "            epoch_bar.set_postfix({\"Average NLL\": f\"{average_nll:.3f}\"})\n",
    "            running_loss = 0.0\n",
    "            running_samples = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f0ae631-2a35-4955-a93a-982c35ccde36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████████████████████████████████████████████████████| 40/40 [00:01<00:00, 35.32it/s, Cumulative LL=-6823475.15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test LL: -682.348\n",
      "Bits per dimension: 1.256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_lls = 0.0\n",
    "\n",
    "    # Wrap the test_dataloader with tqdm progress bar\n",
    "    test_bar = tqdm(test_dataloader, desc=\"Testing\")\n",
    "    for batch, _ in test_bar:\n",
    "        # The circuit expects an input of shape (batch_dim, num_channels, num_variables),\n",
    "        # so we unsqueeze a dimension for the channel.\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Compute the log-likelihoods of the batch\n",
    "        log_likelihoods = circuit(batch)\n",
    "\n",
    "        # Accumulate the log-likelihoods\n",
    "        test_lls += log_likelihoods.sum().item()\n",
    "        \n",
    "        # Update the progress bar with the current cumulative log-likelihood\n",
    "        test_bar.set_postfix({\"Cumulative LL\": f\"{test_lls:.2f}\"})\n",
    "\n",
    "    # Compute average test log-likelihood and bits per dimension\n",
    "    average_ll = test_lls / len(data_test)\n",
    "    bpd = -average_ll / (28 * 28 * np.log(2.0))\n",
    "    print(f\"Average test LL: {average_ll:.3f}\")\n",
    "    print(f\"Bits per dimension: {bpd:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31428e34-6e24-4c21-a7f3-e55214ce4fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784])\n",
      "tensor([[[-596.9652]]], grad_fn=<ToCopyBackward0>)\n",
      "torch.Size([1, 784])\n",
      "tensor([[[-771.1097]]], grad_fn=<ToCopyBackward0>)\n",
      "torch.Size([1, 784])\n",
      "tensor([[[-352.9809]]], grad_fn=<ToCopyBackward0>)\n",
      "torch.Size([1, 784])\n",
      "tensor([[[-832.0175]]], grad_fn=<ToCopyBackward0>)\n",
      "torch.Size([1, 784])\n",
      "tensor([[[-629.8032]]], grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Instantiate the test DataLoader\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256)\n",
    "\n",
    "# Retrieve the first batch from the test DataLoader\n",
    "data_iter = iter(test_dataloader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Extract the first image and its label from the batch\n",
    "sample_image = images[0]\n",
    "sample_label = labels[0]\n",
    "def generate_and_plot_images(circuit, num_images=5):\n",
    "    \"\"\"\n",
    "    Generates and plots images from the probabilistic circuit.\n",
    "    \n",
    "    Args:\n",
    "        circuit: The probabilistic circuit object.\n",
    "        num_images: Number of images to generate.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(10, 2))\n",
    "    for i in range(num_images):\n",
    "        # Sample from the circuit\n",
    "        input_data = torch.tensor(\n",
    "                np.random.randint(0, 256, size=(1, 784), dtype=np.uint8),\n",
    "                dtype=torch.float32  # make sure dtype is compatible with model\n",
    "            )\n",
    "\n",
    "        input_data = images[i].reshape(1,784).to(device)\n",
    "        print(input_data.shape)\n",
    "        sampled_data = circuit.forward(input_data).to(torch.device('cpu'))\n",
    "        print(sampled_data)\n",
    "        # Reshape the sampled data to 28x28 if it's flattened\n",
    "        image = input_data.reshape(28, 28)\n",
    "        # Plot the image\n",
    "        axes[i].imshow(sampled_data.detach().numpy(), cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(\"Generated MNIST-like Images\")\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'circuit' is your trained probabilistic circuit\n",
    "generate_and_plot_images(circuit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "811f7a27-d09b-4996-b8d6-34b3cc6553e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "indices should be either on cpu or on the same device as the indexed tensor (cpu)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m input_tensor = torch.tensor(reshaped_data, dtype=torch.float32)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Pass the tensor to the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m output = \u001b[43mcircuit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/5fa69q8n4lpdkrifly9pgc4gg0fqaqnn-msc-cirkit-dev-env/lib/python3.12/site-packages/cirkit/backend/torch/circuits.py:277\u001b[39m, in \u001b[36mTorchCircuit.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m    265\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Evaluate the circuit layers in forward mode, i.e., by evaluating each layer by\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[33;03m    following the topological ordering.\u001b[39;00m\n\u001b[32m    267\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m \u001b[33;03m            and $K$ is the number of scalars in each output (e.g., the number of classes).\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/5fa69q8n4lpdkrifly9pgc4gg0fqaqnn-msc-cirkit-dev-env/lib/python3.12/site-packages/cirkit/backend/torch/circuits.py:250\u001b[39m, in \u001b[36mAbstractTorchCircuit._evaluate_layers\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate_layers\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor | \u001b[38;5;28;01mNone\u001b[39;00m) -> Tensor:\n\u001b[32m    249\u001b[39m     \u001b[38;5;66;03m# Evaluate layers on the given input\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (O, B, K)\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m y.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/5fa69q8n4lpdkrifly9pgc4gg0fqaqnn-msc-cirkit-dev-env/lib/python3.12/site-packages/cirkit/backend/torch/graph/modules.py:326\u001b[39m, in \u001b[36mTorchDiAcyclicGraph.evaluate\u001b[39m\u001b[34m(self, x, module_fn)\u001b[39m\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# Evaluate the computational graph by following the topological ordering,\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# and by using the book address information to retrieve the inputs to each\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;66;03m# (possibly folded) torch module.\u001b[39;00m\n\u001b[32m    325\u001b[39m module_outputs: \u001b[38;5;28mlist\u001b[39m[Tensor] = []\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_address_book\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/5fa69q8n4lpdkrifly9pgc4gg0fqaqnn-msc-cirkit-dev-env/lib/python3.12/site-packages/cirkit/backend/torch/circuits.py:66\u001b[39m, in \u001b[36mLayerAddressBook.lookup\u001b[39m\u001b[34m(self, module_outputs, in_graph)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(in_graph.shape) != \u001b[32m2\u001b[39m:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     62\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe input to the circuit should have shape (B, D), \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     63\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwhere B is the batch size and D is the number of variables \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     64\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe circuit is defined on\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     65\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m x = \u001b[43min_graph\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscope_idx\u001b[49m\u001b[43m]\u001b[49m.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m layer, (x,)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: indices should be either on cpu or on the same device as the indexed tensor (cpu)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Generate random data with values between 0 and 255, and shape (1, 28, 28)\n",
    "random_data = np.random.randint(0, 256, size=(1, 28, 28), dtype=np.uint8)\n",
    "\n",
    "# Normalize the data to the range [0, 1] if required by the model\n",
    "normalized_data = random_data / 255.0\n",
    "\n",
    "# Reshape the data to match the model's expected input shape (B, D)\n",
    "reshaped_data = normalized_data.reshape(1, 784)  # 1 sample, 784 features\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "input_tensor = torch.tensor(reshaped_data, dtype=torch.float32)\n",
    "\n",
    "# Pass the tensor to the model\n",
    "output = circuit.forward(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ea794-ed67-465a-bc79-3b8ed2e18830",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde03111-6549-4877-b909-6d1e5366d0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
