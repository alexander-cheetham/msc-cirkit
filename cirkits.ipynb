{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b63f493-a57a-4296-858f-0cd442f8b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.templates import data_modalities, utils\n",
    "\n",
    "symbolic_circuit = data_modalities.image_data(\n",
    "    (1, 28, 28),                # The shape of MNIST image, i.e., (num_channels, image_height, image_width)\n",
    "    region_graph='quad-graph',  # Select the structure of the circuit to follow the QuadGraph region graph\n",
    "    input_layer='categorical',  # Use Categorical distributions for the pixel values (0-255) as input layers\n",
    "    num_input_units=64,         # Each input layer consists of 64 Categorical input units\n",
    "    sum_product_layer='cp',     # Use CP sum-product layers, i.e., alternate dense layers with Hadamard product layers\n",
    "    num_sum_units=64,# Each dense sum layer consists of 64 sum units\n",
    "    num_classes=1,\n",
    "    sum_weight_param=utils.Parameterization(\n",
    "        activation='softmax',   # Parameterize the sum weights by using a softmax activation\n",
    "        initialization='normal' # Initialize the sum weights by sampling from a standard normal distribution\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf3eac8-54ed-41c7-bcd7-f4ba3f60bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables: 784\n",
      "\n",
      "Structural properties:\n",
      "  - Smoothness: True\n",
      "  - Decomposability: True\n",
      "  - Structured-decomposability: False\n"
     ]
    }
   ],
   "source": [
    "# Print some information\n",
    "print(f'Number of variables: {symbolic_circuit.num_variables}')\n",
    "print()\n",
    "\n",
    "# Print which structural properties the circuit satisfies\n",
    "print(f'Structural properties:')\n",
    "print(f'  - Smoothness: {symbolic_circuit.is_smooth}')\n",
    "print(f'  - Decomposability: {symbolic_circuit.is_decomposable}')\n",
    "print(f'  - Structured-decomposability: {symbolic_circuit.is_structured_decomposable}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b312e0da-a1d3-4880-a7bb-21237ee3130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set some seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set the torch device to use\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c84d57e-e514-4fc9-95c0-1822710a6ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.38 s, sys: 56.8 ms, total: 1.44 s\n",
      "Wall time: 1.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from cirkit.pipeline import compile\n",
    "circuit = compile(symbolic_circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24abb86d-12ef-42dc-a692-174ff3ae1ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 5725\n",
      "Number of learnable parameters: 25657730\n"
     ]
    }
   ],
   "source": [
    "# Print some statistics\n",
    "num_layers = len(list(symbolic_circuit.layers))\n",
    "print(f\"Number of layers: {num_layers}\")\n",
    "num_parameters = sum(p.numel() for p in circuit.parameters())\n",
    "print(f\"Number of learnable parameters: {num_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd584dc9-16c0-4528-801c-5b5cbbb92768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Load the MNIST data set and data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Flatten the images and set pixel values in the [0-255] range\n",
    "    transforms.Lambda(lambda x: (255 * x.view(-1)).long())\n",
    "])\n",
    "data_train = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "# Instantiate the training and testing data loaders\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, batch_size=256)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256)\n",
    "\n",
    "# Initialize a torch optimizer of your choice,\n",
    "#  e.g., Adam, by passing the parameters of the circuit\n",
    "optimizer = optim.Adam(circuit.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb548216-1b48-4187-80ed-2518deb37fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|███████████████████████████████| 235/235 [00:08<00:00, 27.65it/s, Average NLL=2492.162]\n",
      "Epoch 2/10: 100%|████████████████████████████████| 235/235 [00:07<00:00, 30.13it/s, Average NLL=895.922]\n",
      "Epoch 3/10: 100%|████████████████████████████████| 235/235 [00:07<00:00, 30.15it/s, Average NLL=785.745]\n",
      "Epoch 4/10: 100%|████████████████████████████████| 235/235 [00:07<00:00, 30.18it/s, Average NLL=749.998]\n",
      "Epoch 5/10: 100%|████████████████████████████████| 235/235 [00:07<00:00, 30.18it/s, Average NLL=729.829]\n",
      "Epoch 6/10: 100%|████████████████████████████████| 235/235 [00:07<00:00, 30.09it/s, Average NLL=707.061]\n",
      "Epoch 7/10: 100%|████████████████████████████████| 235/235 [00:07<00:00, 29.45it/s, Average NLL=698.309]\n",
      "Epoch 8/10: 100%|████████████████████████████████| 235/235 [00:08<00:00, 28.57it/s, Average NLL=693.299]\n",
      "Epoch 9/10: 100%|████████████████████████████████| 235/235 [00:08<00:00, 28.55it/s, Average NLL=686.725]\n",
      "Epoch 10/10: 100%|███████████████████████████████| 235/235 [00:08<00:00, 28.60it/s, Average NLL=683.997]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 10\n",
    "step_idx = 0\n",
    "running_loss = 0.0\n",
    "running_samples = 0\n",
    "\n",
    "# Move the circuit to chosen device\n",
    "circuit = circuit.to(device)\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    # Create a tqdm progress bar for the inner loop of the current epoch\n",
    "    epoch_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch_idx+1}/{num_epochs}\")\n",
    "    for i, (batch, _) in enumerate(epoch_bar):\n",
    "        # The circuit expects an input of shape (batch_dim, num_variables)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Compute the log-likelihoods of the batch by evaluating the circuit\n",
    "        log_likelihoods = circuit(batch)\n",
    "\n",
    "        # We take the negated average log-likelihood as loss\n",
    "        loss = -torch.mean(log_likelihoods)\n",
    "        loss.backward()\n",
    "        # Update the parameters of the circuit, as with any other model in PyTorch\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.detach() * len(batch)\n",
    "        running_samples += len(batch)\n",
    "        step_idx += 1\n",
    "\n",
    "        if step_idx % 200 == 0:\n",
    "            average_nll = running_loss / running_samples\n",
    "            # Update the tqdm progress bar with the latest average NLL\n",
    "            epoch_bar.set_postfix({\"Average NLL\": f\"{average_nll:.3f}\"})\n",
    "            running_loss = 0.0\n",
    "            running_samples = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f0ae631-2a35-4955-a93a-982c35ccde36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|                                         | 0/40 [00:00<?, ?it/s, Cumulative LL=-168601.06]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████████| 40/40 [00:01<00:00, 38.57it/s, Cumulative LL=-6823475.15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test LL: -682.348\n",
      "Bits per dimension: 1.256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_lls = 0.0\n",
    "\n",
    "    # Wrap the test_dataloader with tqdm progress bar\n",
    "    test_bar = tqdm(test_dataloader, desc=\"Testing\")\n",
    "    for batch, _ in test_bar:\n",
    "        # The circuit expects an input of shape (batch_dim, num_channels, num_variables),\n",
    "        # so we unsqueeze a dimension for the channel.\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Compute the log-likelihoods of the batch\n",
    "        log_likelihoods = circuit(batch)\n",
    "\n",
    "        # Accumulate the log-likelihoods\n",
    "        test_lls += log_likelihoods.sum().item()\n",
    "        \n",
    "        # Update the progress bar with the current cumulative log-likelihood\n",
    "        test_bar.set_postfix({\"Cumulative LL\": f\"{test_lls:.2f}\"})\n",
    "\n",
    "    # Compute average test log-likelihood and bits per dimension\n",
    "    average_ll = test_lls / len(data_test)\n",
    "    bpd = -average_ll / (28 * 28 * np.log(2.0))\n",
    "    print(f\"Average test LL: {average_ll:.3f}\")\n",
    "    print(f\"Bits per dimension: {bpd:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31428e34-6e24-4c21-a7f3-e55214ce4fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "indices should be either on cpu or on the same device as the indexed tensor (cpu)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Assuming 'circuit' is your trained probabilistic circuit\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mgenerate_and_plot_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 31\u001b[0m, in \u001b[0;36mgenerate_and_plot_images\u001b[0;34m(circuit, num_images)\u001b[0m\n\u001b[1;32m     29\u001b[0m input_data \u001b[38;5;241m=\u001b[39m images[i]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m784\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 31\u001b[0m sampled_data \u001b[38;5;241m=\u001b[39m \u001b[43mcircuit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(sampled_data)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Reshape the sampled data to 28x28 if it's flattened\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cirkit/lib/python3.10/site-packages/cirkit/backend/torch/circuits.py:277\u001b[0m, in \u001b[0;36mTorchCircuit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    265\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate the circuit layers in forward mode, i.e., by evaluating each layer by\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m    following the topological ordering.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m            and $K$ is the number of scalars in each output (e.g., the number of classes).\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cirkit/lib/python3.10/site-packages/cirkit/backend/torch/circuits.py:250\u001b[0m, in \u001b[0;36mAbstractTorchCircuit._evaluate_layers\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate_layers\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# Evaluate layers on the given input\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (O, B, K)\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/cirkit/lib/python3.10/site-packages/cirkit/backend/torch/graph/modules.py:326\u001b[0m, in \u001b[0;36mTorchDiAcyclicGraph.evaluate\u001b[0;34m(self, x, module_fn)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# Evaluate the computational graph by following the topological ordering,\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# and by using the book address information to retrieve the inputs to each\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# (possibly folded) torch module.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m module_outputs: \u001b[38;5;28mlist\u001b[39m[Tensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_address_book\u001b[38;5;241m.\u001b[39mlookup(module_outputs, in_graph\u001b[38;5;241m=\u001b[39mx):\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m         (output,) \u001b[38;5;241m=\u001b[39m inputs\n",
      "File \u001b[0;32m~/.conda/envs/cirkit/lib/python3.10/site-packages/cirkit/backend/torch/circuits.py:66\u001b[0m, in \u001b[0;36mLayerAddressBook.lookup\u001b[0;34m(self, module_outputs, in_graph)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(in_graph\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input to the circuit should have shape (B, D), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere B is the batch size and D is the number of variables \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe circuit is defined on\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m     )\n\u001b[0;32m---> 66\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43min_graph\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscope_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m layer, (x,)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: indices should be either on cpu or on the same device as the indexed tensor (cpu)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Instantiate the test DataLoader\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256)\n",
    "\n",
    "# Retrieve the first batch from the test DataLoader\n",
    "data_iter = iter(test_dataloader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Extract the first image and its label from the batch\n",
    "sample_image = images[0]\n",
    "sample_label = labels[0]\n",
    "def generate_and_plot_images(circuit, num_images=5):\n",
    "    \"\"\"\n",
    "    Generates and plots images from the probabilistic circuit.\n",
    "    \n",
    "    Args:\n",
    "        circuit: The probabilistic circuit object.\n",
    "        num_images: Number of images to generate.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(10, 2))\n",
    "    for i in range(num_images):\n",
    "        # Sample from the circuit\n",
    "        input_data = torch.tensor(\n",
    "                np.random.randint(0, 256, size=(1, 784), dtype=np.uint8),\n",
    "                dtype=torch.float32  # make sure dtype is compatible with model\n",
    "            )\n",
    "\n",
    "        input_data = images[i].reshape(1,784).to(device)\n",
    "        print(input_data.shape)\n",
    "        sampled_data = circuit.forward(input_data).to(torch.device('cpu'))\n",
    "        print(sampled_data)\n",
    "        # Reshape the sampled data to 28x28 if it's flattened\n",
    "        image = input_data.reshape(28, 28)\n",
    "        # Plot the image\n",
    "        axes[i].imshow(image, cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(\"Generated MNIST-like Images\")\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'circuit' is your trained probabilistic circuit\n",
    "generate_and_plot_images(circuit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811f7a27-d09b-4996-b8d6-34b3cc6553e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Generate random data with values between 0 and 255, and shape (1, 28, 28)\n",
    "random_data = np.random.randint(0, 256, size=(1, 28, 28), dtype=np.uint8)\n",
    "\n",
    "# Normalize the data to the range [0, 1] if required by the model\n",
    "normalized_data = random_data / 255.0\n",
    "\n",
    "# Reshape the data to match the model's expected input shape (B, D)\n",
    "reshaped_data = normalized_data.reshape(1, 784)  # 1 sample, 784 features\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "input_tensor = torch.tensor(reshaped_data, dtype=torch.float32)\n",
    "\n",
    "# Pass the tensor to the model\n",
    "output = circuit.forward(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ea794-ed67-465a-bc79-3b8ed2e18830",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde03111-6549-4877-b909-6d1e5366d0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
